@article{boettiger2017,
  title = {An {{Introduction}} to {{Rocker}}: {{Docker Containers}} for {{R}}},
  shorttitle = {An {{Introduction}} to {{Rocker}}},
  author = {Boettiger, Carl and Eddelbuettel, Dirk},
  year = {2017},
  journal = {The R Journal},
  volume = {9},
  number = {2},
  pages = {527--536},
  issn = {2073-4859},
  doi = {10/ghgdtz},
  urldate = {2024-11-02},
  langid = {english}
}

@book{chacon2014,
  title = {Pro {{Git}}},
  author = {Chacon, S and Straub, B},
  year = {2014},
  edition = {2nd},
  publisher = {Apress}
}

@article{merkel2014,
  title = {Docker: {{Lightweight}} Linux Containers for Consistent Development and Deployment},
  author = {Merkel, Dirk},
  year = {2014},
  journal = {Linux journal},
  volume = {2014},
  number = {239},
  pages = {2}
}

@article{nosek2019,
  title = {Preregistration Is Hard, and Worthwhile},
  author = {Nosek, Brian A. and Beck, Emorie D. and Campbell, Lorne and Flake, Jessica K. and Hardwicke, Tom E. and Mellor, David T. and {van 't Veer}, Anna E. and Vazire, Simine},
  year = {2019},
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {10},
  pages = {815--818},
  issn = {1364-6613},
  urldate = {2025-07-01},
  abstract = {Preregistration clarifies the distinction between planned and unplanned research by reducing unnoticed flexibility. This improves credibility of findings and calibration of uncertainty. However, making decisions before conducting analyses requires practice. During report writing, respecting both what was planned and what actually happened requires good judgment and humility in making claims.},
  keywords = {confirmatory research,exploratory research,preregistration,reproducibility,transparency}
}

@article{sandve2013,
  title = {Ten Simple Rules for Reproducible Computational Research},
  author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
  year = {2013},
  journal = {PLOS Computational Biology},
  volume = {9},
  number = {10},
  pages = {e1003285},
  issn = {1553-7358},
  urldate = {2025-07-01},
  langid = {english},
  keywords = {Archives,Computer and information sciences,Computer applications,Genome analysis,Habits,Replication studies,Reproducibility,Source code}
}

@misc{scheidegger2025,
  title = {Quarto},
  author = {Scheidegger, Carlos and Woodhull, Gordon and Dervieux, Christophe and Teague, Charles and Allaire, J J and Xie, Yihui},
  year = {2025},
  howpublished = {Posit, PBC}
}

@article{strand2025,
  title = {Error Tight: Exercises for Lab Groups to Prevent Research Mistakes},
  shorttitle = {Error Tight},
  author = {Strand, Julia F.},
  year = {2025},
  journal = {Psychological Methods},
  volume = {30},
  number = {2},
  pages = {416--424},
  issn = {1082-989X},
  urldate = {2025-04-18},
  abstract = {Scientists, being human, make mistakes. We transcribe things incorrectly, we make errors in our code, and we intend to do things and then forget. The consequences of errors in research may be as minor as wasted time and annoyance, but may be as severe as losing months of work or having to retract an article. The purpose of this tutorial is to help lab groups identify places in their research workflow where errors may occur and identify ways to avoid them. To do this, this article applies concepts from human factors research on how to create lab cultures and workflows that are intended to minimize errors. This article does not provide a one-size-fits-all set of guidelines for specific practices to use (e.g., one platform on which to backup data); instead, it gives examples of ways that mistakes can occur in research along with recommendations for systems that avoid and detect them. This tutorial is intended to be used as a discussion prompt prior to a lab meeting to help researchers reflect on their own processes and implement safeguards to avoid future errors. (PsycInfo Database Record (c) 2025 APA, all rights reserved) (Source: journal abstract) Translational Abstract: Everyone makes mistakes. In science, mistakes can occur in many ways: Researchers may transcribe things incorrectly, make typos when writing code to analyze data, forget to do something they intended to, and so forth. These mistakes may simply waste time or require redoing work, but in more serious cases, they can ruin an experiment or lead to false conclusions. However, learning how to avoid errors in research isn't a standard part of training. This tutorial is intended to help lab groups identify places in the research process where errors may occur and identify ways to avoid them. To do so, this article draws on lessons from high-risk fields such as aviation, surgery, and construction, all of which have developed explicit, practical strategies to reduce mistakes on the job. This tutorial is intended to be used as a discussion prompt before a lab meeting to help researchers reflect on their own processes and implement safeguards to avoid future errors. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
  copyright = {{\copyright} 2023, American Psychological Association},
  langid = {english},
  keywords = {Concepts,Data Sets,Errors,Experimental Laboratories,Forgetting,Methodology,Scientists}
}

@article{tackett2017,
  title = {It's Time to Broaden the Replicability Conversation: {{Thoughts}} for and from Clinical Psychological Science},
  author = {Tackett, Jennifer L and Lilienfeld, Scott O and Patrick, Christopher J and Johnson, Sheri L and Krueger, Robert F and Miller, Joshua D and Oltmanns, Thomas F and Shrout, Patrick E},
  year = {2017},
  journal = {Perspectives on Psychological Science},
  volume = {12},
  number = {5},
  pages = {742--756},
  issn = {1745691613489},
  doi = {10.1177/1745691617690042},
  abstract = {Psychology is in the early stages of examining a crisis of replicability stemming from several high-profile failures to replicate studies in experimental psychology. This important conversation has largely been focused on social psychology, with some active participation from cognitive psychology. Nevertheless, several other major domains of psychological science---including clinical science---have remained insulated from this discussion. The goals of this article are to (a) examine why clinical psychology and allied fields, such as counseling and school psychology, have not been central participants in the replicability conversation; (b) review concerns and recommendations that are less (or more) applicable to or appropriate for research in clinical psychology and allied fields; and (c) generate take-home messages for scholars and consumers of the literature in clinical psychology and allied fields, as well as reviewers, editors, and colleagues from other areas of psychological science.}
}

@book{turrell2025,
  title = {Python for {{Data Science}}},
  author = {Turrell, Arthur and Monticone, Pietro and Akyol, Zeki and Holman, Josh and Huang, Yiben},
  year = {2025}
}

@misc{ushey2025,
  title = {Renv: {{Project Environments}}},
  author = {Ushey, Kevin and Wickham, Hadley},
  year = {2025},
  annotation = {R package version 1.1.4, https://rstudio.github.io/renv}
}

@article{vantil2025,
  title = {Open Science Training in {{APA-accredited}} Clinical Psychology Programs: {{A}} Registered Report},
  author = {Van Til, Kaela and Phillips, Nathaniel and Du, Tianwei and Rose, Leigha and Miller, Joshua and Lynam, Donald},
  year = {2025},
  journal = {Clinical Psychological Science}
}

@book{wickham2023,
  title = {R for {{Data Science}}},
  author = {Wickham, Hadley and {\c C}etinkaya-Rundel, Mine and Grolemund, Garrett},
  year = {2023},
  edition = {2nd},
  publisher = {O'Reilly Media}
}
