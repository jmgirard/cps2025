---
title: "Computational and Pedagogical Tools for Open Science:\\\nCommentary on Van Til et al. (2025)"
shorttitle: "Tools for Open Science"
author:
  - name: Jeffrey M. Girard
    corresponding: true
    orcid: 0000-0002-7359-3746
    email: jmgirard@ku.edu
    affiliations:
      - name: University of Kansas
        department: Department of Psychology
        address: 1415 Jayhawk Blvd (Room 426)
        city: Lawrence
        region: KS
        country: USA
        postal-code: 66045
        role: 
          - conceptualization
          - writing
author-note:
  status-changes: 
    affiliation-change: ~
    deceased: ~
  disclosures:
    study-registration: ~
    data-sharing: "Supplemental materials are available from: <https://github.com/jmgirard/cps2025>"
    related-report: ~
    conflict-of-interest: ~
    financial-support: ~
    gratitude: ~
    authorship-agreements: ~
abstract: "Clinical psychology has lagged in adopting open science practices, despite student interest and widespread recognition of their value. Building on Van Til et al. (2025)'s findings of limited exposure in doctoral training, I argue that transparency requires more than brief exposure; it demands scaffolded, intentional instruction. Integrating preregistration, code sharing, and reproducible workflows early in the curriculum is essential. I describe how computational tools can support this shift, highlight institutional and pedagogical barriers, and call for faculty development and program-level support to ensure open science becomes a shared norm, not just an ideal."
keywords: [commentary, open science, graduate training, research methods]
bibliography: bibliography.bib
format:
  apaquarto-pdf:
    documentmode: man
  apaquarto-docx:
    blank-lines-above-title: 4
    blank-lines-above-author-note: 9
---

Open science offers powerful tools for transparency and rigor, yet clinical psychology has lagged behind other subfields in embedding them in graduate training. Much of this lag reflects distinct headwinds: handling sensitive patient data, coordinating with clinics and IRBs, and teaching in highly individualized settings [@tackett2017]. Still, these factors do not fully explain the gap. Core practices such as preregistration, code sharing, and reproducible workflows can be taught and adopted despite them.

Drawing on syllabus audits and graduate-student surveys, @vantilinpress document that open-science training is present in some U.S. programs but uneven overall. Readings and occasional assignments appear, yet hands-on opportunities to practice core behaviors are scarce. Students are generally interested, but many remain unfamiliar with practices such as registered reports.

Writing as a clinical psychologist with a strong computational focus, I take an opinionated stance: open science will not advance through exposure to principles alone. Graduate programs must provide sustained, scaffolded, and hands-on training that turns open-science ideals into routine habits and integrates modern computational tools. Building on Van Til and colleague's findings, I argue that progress depends on three interconnected fronts: pedagogical scaffolding, computational infrastructure, and cultural alignment.

# Building Open Science into Graduate Training

If psychology is to prepare the next generation of scientifically rigorous researchers, open-science principles must be woven into the curriculum from the very beginning of graduate education. The first semester plays a disproportionate role in shaping a student's identity as a researcher [@bruss1993; @cruess2015]. Early coursework in methods and statistics not only teaches design and analysis but also transmits the norms that define what counts as rigorous and responsible science. Programs that postpone or marginalize open-science instruction implicitly signal that transparency is peripheral rather than central to good research practice.

An effective approach should scaffold open-science training across the full arc of graduate development. Introductory methods courses can present the rationale for transparency and provide structured preregistration exercises that help students distinguish between exploratory and confirmatory inquiry. Early statistics courses can pair reproducible code with transparent documentation, allowing students to practice the full cycle of analysis, interpretation, and reporting. As students progress, they should apply these skills to projects grounded in their own interests: designing a study, preregistering hypotheses, analyzing original data, and depositing materials in an open repository. By the dissertation stage, students ought to be fluent in core open-research workflows such as preregistration, code and data sharing, and reproducible reporting, while optionally experimenting with registered reports, replication studies, or version-controlled collaboration.

Because many programs deliver research and statistics training to students from multiple subfields [not solely clinical\; @aiken2008], models of instructional collaboration should be flexible. Pairing quantitative instruction with clinically oriented case examples, guest mini-modules, or joint office hours can capture much of the benefit of "co-teaching" without requiring full, dual-instructor courses. In programs where a dedicated co-taught sequence is feasible, pairing clinical expertise with statistical pedagogy can make open-science practices feel directly relevant to clinical decision-making; where it is not, lighter-weight coordination (e.g., shared assignments or lab meetings that bridge courses and practica) can achieve similar ends.

Adoption also depends on open educational resources [OER\; @wiley2014] that lower preparation costs and standardize quality. Reusable, CC-licensed syllabi, assignment templates (e.g., preregistration exercises, cose-based lab reports), exemplar repositories with de-identified or synthetic data, and instructor guides can make it far easier for time-limited faculty to integrate open-science content. Scientific organizations and disciplinary societies are well positioned to underwrite and curate these materials (e.g., competitive micro-grants for OER development, peer-reviewed teaching portals, or "starter kits" aligned to accreditation competencies). Department-level incentives (e.g., recognizing OER creation as scholarly contribution, providing small teaching fellowships for graduate assistants to localize materials) further increase the odds of durable adoption.

Of course, this scaffolding must be realistic about the demands on both students and faculty. First-year students already face steep learning curves in statistics, research design, and clinical work. Adding open-science expectations without sufficient structure risks overwhelming rather than empowering them. Likewise, many instructors support transparency in principle but lack the time, training, or institutional backing to redesign courses and assignments. Addressing these challenges means providing pedagogical supports, not lowering expectations: template-based preregistrations, staged code assignments, co-taught modules that pair statistical and clinical expertise, and faculty development resources that make curricular change feasible. In short, scaffolding must apply not only to students' learning but to instructors' adoption of new practices. Embedding transparency in graduate education therefore depends on both sound pedagogy and the technical infrastructure that makes open practices practical and sustainable.

# Tools and Workflows that Empower Transparency

Open science is grounded in values and policies, but it is sustained by tools and technologies that make those values actionable. The tools students learn early on often become the scaffolding for their scientific careers; ensuring they support reproducibility, transparency, and collaboration is therefore essential.

## Code-Based Workflows

Perhaps the most transformative shift in recent research practice has been the move from costly, point-and-click statistical software to open-source, code-based workflows. Whether students use R, Python, or another programming language, writing and documenting analytic scripts fosters a level of transparency and reproducibility that manual workflows cannot match. Analyses encoded in readable, versioned scripts can be reviewed, rerun, and built upon by others. The analysis itself becomes an object of scientific discourse, not a black box [@sandve2013].

There is no justifiable scientific reason to withhold analytic code from a published study. While data may need protection, code is part of the method. Yet many students are never taught to view code as a scientific product. This gap represents a clear target for curricular reform.

## Literate Programming

Literate programming treats code and narrative text as equally important parts of scientific communication. Tools like Quarto, an open-source publishing system, allow researchers to seamlessly combine text, code, and output (figures, tables, diagnostics) in one document [@scheidegger2025]. Quarto supports citations, equations, and cross-references, producing reports, slides, websites, or manuscripts that are fully reproducible.[^1]

[^1]: This article was created using Quarto. See [github.com/jmgirard/cps2025](https://github.com/jmgirard/cps2025) for the code.

In graduate courses, literate programming encourages students to document their reasoning, explain decisions, and produce reports that can be rerun at any time. This approach is ideal for methods assignments, replication projects, and theses. It also reflects how high-quality research is increasingly disseminated---not just through articles but through richly documented supplements that promote transparency and cumulative science. These same practices prepare students to collaborate on interdisciplinary research teams and open doors to roles beyond academia, including in data science and applied research, where reproducible workflows are essential.

## Beyond the Basics

While more advanced, three computational frameworks further enhance transparency and reproducibility. *Version control* (e.g., Git, GitHub) saves timestamped snapshots of your code and writing as you work, so you can see what changed and restore earlier versions when needed [@chacon2014]. It also lets multiple people collaborate without overwriting one another, creating a clear, shareable history that supports open review and transparency. GitHub can also host simple, version-controlled websites for free, making the creation of a personal researcher site an excellent student project.

*Dependency management* (e.g., renv in R) snapshots the exact package versions used in an analysis and restores them on another machine, preventing "it worked last year" failures when updates break code [@ushey2025]. By pinning versions and documenting the software state, readers can rerun analyses under the same conditions, strengthening reproducibility.

*Containerization tools* (e.g., Docker) encapsulate the entire computational environment---operating system, dependencies, packages, and code---into a portable image that runs identically across systems [@merkel2014;@boettiger2017]. Containers make complex pipelines shareable and archivable (e.g., alongside a manuscript or preregistration), enabling exact re-execution years later and supporting rigorous replication and extension.

Though more technical, these methods future-proof scientific work and are increasingly vital for interdisciplinary and collaborative projects.

## Tools as Values in Action

These tools do more than ensure technical accuracy---they teach a way of thinking. Code-based workflows, literature programming, and environment management foster habits of documentation, foresight, and humility. They signal that science is not only about reaching results but about making the process intelligible and reviewable. By teaching these tools explicitly and modeling their use, programs can help students build workflows that are not only reproducible but also resilient and collaborative. In a world where scientific claims face growing scrutiny, these capacities are not optional; they are part of what it means to do science well.

Yet even the best tools cannot overcome the ethical, institutional, and cultural barriers that shape how openness is practiced. These realities are particularly salient in clinical psychology.

# Ethical and Institutional Realities in Clinical Training

Clinical psychology occupies a distinctive position at the intersection of scientific training and professional regulation. Because most programs operate under formal accreditation standards, accrediting bodies such as the American Psychological Association (APA) and the Psychological Clinical Science Accreditation System (PCSAS) exert substantial influence over curricular priorities. Depending on how they define competencies in research and ethics, these organizations can either facilitate or hinder the adoption of open-science practices. Clinical psychology thus functions as a model-setting domain for embedding transparency within professional education, where accreditation requirements, competency benchmarks, and supervisory structures can translate principles into enforceable expectations.

At the same time, clinical training entails challenges few other areas face. Ethical and legal imperatives of patient confidentiality complicate sharing of raw data, preregistration of sensitive protocols, and replication of clinical trials [e.g., @ness2007]. Research partnerships with hospitals, clinics, and community agencies are governed by data-use agreements that may limit dissemination. Students must also balance research with coursework and intensive clinical practica, leaving limited bandwidth to master new computational tools. These constraints can make open-science ideals seem aspirational rather than achievable.

Yet the very features that complicate openness also create opportunities for innovation. Clinical psychology can pioneer frameworks for *ethical transparency* that balance rigor with privacy. Examples include using de-identified or synthetic datasets for teaching, secure data enclaves for authorized replication, and preregistration formats that allow embargoes until sensitive work concludes [e.g., @howison2024; @liu2025]. Because clinical training already integrates ethics and reflective supervision, open-science competencies can be framed as extensions of professional responsibility rather than external mandates. Linking transparency to ethical conduct reinforces its relevance to both science and care.

Even with these structures in place, real research is messy. Students learning statistics may lack the knowledge to preregister analyses in full detail, and projects often evolve in response to data patterns or reviewer feedback [@nosek2019;@simmons2021]. Rigid preregistration can backfire if it fosters box-checking over critical reflection. Instead, students should be taught to view preregistration as a flexible tool for clarifying thought, not constraining it: distinguishing exploratory from confirmatory work, documenting changes transparently, and recognizing that deviation is part of learning. Similarly, registered reports should be seen not as universal solutions but as gold standards for certain kinds of confirmatory research.

Another subtle barrier is the culture of perfectionism that pervades many graduate programs. When open science is framed in idealized terms, students may equate rigor with flawlessness and transparency with self-exposure. For trainees already navigating imposter syndrome, this can breed paralysis rather than curiosity. Faculty (in both teaching and research roles) can counter this by modeling vulnerability---sharing imperfect code, troubleshooting errors in front of students, and emphasizing that open science is about clarity, not perfection [@strand2025].

Finally, not everyone shares the same values or priorities that motivate open-science advocacy [e.g., @bazzoli2022]. While many view transparency as central to integrity, others prioritize innovation, idiographic understanding, or patient impact. Some are skeptical that preregistration and data sharing improves understanding; others worry these practices might disadvantage those working with sensitive data, limited resources, or qualitative methods. These differences reflect genuine pluralism in what scientists see as the purpose of research. Because clinical psychology operates at the nexus of scientific and professional ethics, conversations about open science must engage (rather than override) this diversity of commitments. Promoting openness, therefore, is as much a cultural project as a technical or pedagogical one. It requires empathy, dialogue, and respect for principled disagreement.

# Conclusion

Van Til and colleagues have provided a valuable service by empirically documenting what many open-science advocates in clinical psychology have long suspected: while interest in transparency and rigor is growing, graduate training has yet to catch up. Their registered report offers both a clear-eyed assessment of the current state and a hopeful reminder that students are eager to engage when given the opportunity.

But awareness is only the first step. For open science to take root in clinical psychology, programs must invest in the pedagogy, infrastructure, and culture that make transparency not just possible, but normative. That means treating reproducibility as a core competency, integrating code-based workflows and preregistration early in the curriculum, supporting faculty with resources and time, and acknowledging that transparency will look different across projects. Imperfection is not a flaw but a feature of honest science.

Open science is a practice we choose, repeatedly, in the face of complexity and constraint. It is a commitment to make our work intelligible to others and accountable to the communities we serve. For clinical psychological science, that commitment is essential---and so is the work of teaching it. A sustainable culture of open science begins in the classroom, is reinforced through mentorship, and is sustained by institutions that value good science not only for its outcomes but for its openness in process.

# Acknowledgments

This manuscript was written with the assistance of a large language model. The author has independently verified the accuracy, validity, and appropriateness of all substantive content.

# Author Contributions

Conceptualization: J. Girard; Writing -- Original Draft Preparation: J. Girard.

# Conflicts of Interest

The author declares that there were no conflicts of interest with respect to the authorship or the publication of this article.

# References

::: {#refs}
:::
