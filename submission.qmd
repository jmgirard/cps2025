---
title: "Computational and Pedagogical Tools for Open Science:\\\nCommentary on Van Til et al. (2025)"
shorttitle: "Tools for Open Science"
word-count: true
author:
  - name: Jeffrey M. Girard
    corresponding: true
    orcid: 0000-0002-7359-3746
    email: jmgirard@ku.edu
    affiliations:
      - name: University of Kansas
        department: Department of Psychology
        address: 1415 Jayhawk Blvd (Room 426)
        city: Lawrence
        region: KS
        country: USA
        postal-code: 66045
        role: 
          - conceptualization
          - writing
author-note:
  status-changes: 
    affiliation-change: ~
    deceased: ~
  disclosures:
    study-registration: ~
    data-sharing: "Supplemental materials are available from: <https://github.com/jmgirard/cps2025>"
    related-report: ~
    conflict-of-interest: ~
    financial-support: ~
    gratitude: ~
    authorship-agreements: ~
abstract: "Clinical psychology has lagged in adopting open science practices, despite student interest and widespread recognition of their value. Building on Van Til et al. (2025)'s findings of limited exposure in doctoral training, I argue that transparency requires more than brief exposure; it demands scaffolded, intentional instruction. Integrating preregistration, code sharing, and reproducible workflows early in the curriculum is essential. I describe how computational tools can support this shift, highlight institutional and pedagogical barriers, and call for faculty development and program-level support to ensure open science becomes a shared norm, not just an ideal."
keywords: [commentary, open science, graduate training, research methods]
bibliography: bibliography.bib
format:
  apaquarto-docx:
    blank-lines-above-title: 4
    blank-lines-above-author-note: 10
  apaquarto-html: default
  apaquarto-typst: default
  apaquarto-pdf:
    documentmode: man
---

Open science offers powerful tools for transparency and rigor, yet clinical psychology has lagged behind other subfields in embedding them in graduate training. Much of this lag reflects distinct headwinds: handling sensitive patient data, coordinating with clinics and IRBs, and teaching in highly individualized settings [@tackett2017]. Still, these factors do not fully explain the gap: core practices such as preregistration, code sharing, and reproducible workflows can be taught and adopted despite them.

Drawing on syllabus audits and graduate-student surveys, @vantil2025 document that open-science training is present in some U.S. programs but uneven overall. Readings and occasional assignments appear, yet hands-on opportunities to practice core behaviors are scarce; students report interest, but unfamiliarity with practices like registered reports remains common.

Writing as a clinical psychologist with a strong computational focus, I take an opinionated stance: open science will not advance through exposure to principles alone. Graduate programs should provide sustained, scaffolded, and hands-on training that turns open-science ideals into routine habits and integrates modern computational tools. My aim in this commentary is to advance that agenda and spark discussion.

# The Training Students Deserve

Across psychological science, endorsement of transparency and rigor is high, but graduate curricula too rarely convert those commitments into routine practice. To make openness the default, programs should embed it from the outset and scaffold it across developmental stages—emphasizing hands-on, reproducible workflows rather than one-off exposure to concepts. Writing as a clinical psychologist with a computational focus, I argue for sustained, practice-based training that treats open science as core professional preparation rather than an optional add-on.

If psychology is to prepare the next generation of scientifically rigorous researchers, open-science principles must be woven into the curriculum from the very beginning of graduate study. The first semester of a program plays a disproportionate role in shaping a student’s identity as a researcher (cite). Early coursework in methods and statistics not only teaches design and analysis but also transmits the norms that define what counts as rigorous and responsible science. Programs that postpone or marginalize open-science instruction implicitly signal that transparency is peripheral rather than central to good research practice.

An effective approach should scaffold open-science training across the full arc of graduate development. Introductory methods courses can present the rationale for transparency and provide structured preregistration exercises that help students distinguish between exploratory and confirmatory inquiry. Early statistics courses can pair reproducible code with transparent documentation, allowing students to practice the full cycle of analysis, interpretation, and reporting. As students progress, they should apply these skills to projects grounded in their own interests—designing a study, preregistering hypotheses, analyzing original data, and depositing materials in an open repository. By the dissertation stage, students ought to be fluent in core open-research workflows such as preregistration, code and data sharing, and reproducible reporting, while optionally experimenting with registered reports, replication studies, or version-controlled collaboration.

Such integration demands institutional coordination and mentorship. Open science cannot be reduced to a single lecture or optional unit; it must be threaded through coursework, supervision, and evaluation. Faculty need not be experts in every open-science tool to serve as models. When instructors and research mentors make their analytic reasoning visible—by documenting decisions, reflecting on errors, or sharing the evolution of a project—they convey that rigor and transparency are active, learnable processes rather than static ideals. Programs that normalize this openness across training contexts are more likely to produce scholars who treat transparency as the default mode of scientific work.

# Clinical Challenges and Opportunities

While the principles above apply to all of psychology, clinical psychology occupies a distinctive position at the intersection of scientific training and professional regulation. Because most clinical programs operate under formal accreditation standards, accrediting bodies such as the American Psychological Association (APA) and the Psychological Clinical Science Accreditation System (PCSAS) exert substantial influence over curricular priorities. Depending on how they define competencies in research and ethics, these organizations can either facilitate or hinder the adoption of open-science practices. Consequently, clinical psychology functions as an especially consequential model-setting domain for embedding transparency within professional education, where accreditation requirements, competency benchmarks, and supervisory structures can translate general principles into enforceable expectations and replicable training routines.

At the same time, clinical training entails challenges that few other areas of psychology face. The ethical and legal imperatives of patient confidentiality complicate the sharing of raw data, preregistration of sensitive protocols, and replication of clinical trials. Research is often conducted in partnership with hospitals, clinics, and community agencies, each governed by institutional review boards and data-use agreements that restrict dissemination. Students must also divide their time between research, coursework, and intensive clinical practica, leaving limited bandwidth for mastering new computational or documentation tools. These constraints can make open-science ideals seem aspirational rather than achievable within applied clinical settings.

Yet the very features that complicate openness also create opportunities for innovation. Clinical psychology is well positioned to pioneer frameworks for *ethical transparency*—approaches that balance rigor with protection of privacy. Examples include the use of de-identified or synthetic datasets for teaching and secondary analysis, secure data enclaves for authorized replication, and preregistration formats that allow embargoes until sensitive work concludes. Because clinical training already integrates ethics, evidence-based practice, and reflective supervision, open-science competencies can be framed as extensions of professional responsibility rather than as external mandates. Linking transparency to ethical conduct reinforces its relevance to both scientific and clinical excellence.

Ultimately, the integration of open-science principles within clinical psychology will depend on structural support as well as individual modeling. Accrediting bodies can update competency standards to include reproducible research workflows and transparent documentation practices. Training directors can align course sequences so that students learn privacy-preserving data management alongside open-science reasoning. Faculty mentors can demonstrate how to navigate ambiguity, sharing both successes and setbacks in implementing transparency under real-world constraints. In doing so, clinical psychology can lead the discipline in demonstrating that openness and confidentiality are not opposing values but complementary commitments to the integrity of both science and care.

# Tools that Empower Transparent Science

Open science is grounded in values and policies, but it is also sustained by tools and technologies that allow researchers to put those principles into practice. Tools shape behavior, lower (or raise) barriers to entry, and implicitly communicate what kinds of work are expected or legitimate. For graduate students in clinical psychology, the tools they learn to use early on will often become the scaffolding for their scientific careers. Ensuring that those tools support reproducibility, transparency, and reusability is therefore an essential part of research training.

## Code-Based Workflows

Perhaps the most transformative shift in research practice over the past decade has been the move from costly point-and-click statistical software to scripted, code-based workflows using open-source programming languages. Whether students use R, Python, or another programming language, learning to write and document their own data processing and analysis scripts fosters a degree of transparency and reproducibility that manual workflows simply cannot match. When analyses are encoded in readable, versioned scripts, others can review them, rerun them, and learn from them. The analysis becomes an object of scientific discourse, not a black box [@sandve2013].

There is no justifiable scientific reason to withhold analytic code from a published study. While raw data may need to be protected for legal or ethical reasons, the code used to analyze that data is part of the method. Omitting it is equivalent to describing a laboratory procedure vaguely or inaccurately. Yet, despite this, many papers still do not include analysis code, and many students are never taught to think of code as a scientific product in its own right. This gap represents a clear area for curricular reform.

## Literate Programming

Literate programming is a paradigm that treats code and narrative text as equally important parts of scientific communication, allowing researchers to explain what their code does and why within a single, coherent document. Tools like Quarto, an open-source publishing system, offer a compelling solution for students and instructors alike [@scheidegger2025]. Quarto enables researchers to seamlessly combine text, code (in R, Python, Julia, or Observable), and output (e.g., figures, tables, diagnostics) in one self-contained document. It supports equations, citations, and cross-references with output formats including reports, slideshows, websites, blogs, books, and articles.[^1] This integrated approach facilitates reproducibility and teaches students to communicate their workflows clearly and cohesively, bridging the gap between technical detail and scientific storytelling.

[^1]: This article was created using Quarto. See [github.com/jmgirard/cps2025](https://github.com/jmgirard/cps2025) for the code.

In a graduate course, literate programming encourages students to document their thought process, explain decisions, and produce reports that can be rerun at any time. This format works especially well for method assignments, replication projects, and theses. It also reflects how high-quality research is increasingly shared---not just through articles and talks, but through richly documented supplemental materials that promote open science. These same practices prepare students to collaborate on interdisciplinary research teams and open doors to roles beyond academia, including in data science and applied research, where reproducible workflows are essential.

## Beyond the Basics

While beyond the scope of most graduate-level methods courses in clinical psychology, three frameworks from computer science can support robust, transparent, and portable workflows. Though less commonly taught in psychology, these approaches align with open science values and are especially useful for students with more programming experience, such as those developing shared codebases, contributing to interdisciplinary projects, or managing evolving analytic pipelines.

Version control systems help students track changes, recover earlier versions, and collaborate without overwriting one another’s work. They create a structured record of a project's development, making it easier to understand decisions and revert if needed. Although most commonly used for code, version control also applies to manuscripts, analysis plans, and other evolving documents. This makes it valuable not only for technical work but for any collaborative or iterative writing process. \textit{Git} is the most widely used system, and platforms like GitHub.com support sharing, coordination, and structured discussion of revisions [@chacon2014].

Dependency management tools ensure that code continues to run reliably even as software packages change. Analyses often depend on specific package versions, and updates can cause code to fail or yield different results. Without a way to restore the original setup, reproducing results becomes difficult. For students working in R, \textit{renv} makes it easy to save and later recreate the package environment used in a project [@ushey2025]. This is critical for open science, where reproducibility depends on preserving not just the code, but also the conditions under which it ran.

Containerization goes further by capturing the entire computational environment, including the operating system, packages, and code. This allows projects to run exactly as they did originally, eliminating the common "it worked on my computer" problem. Tools like \textit{Docker} let students bundle all required components into portable environments that run consistently across systems [@merkel2014]. Containerization is especially helpful for sharing complex projects, archiving long-term analyses, or supporting reproducibility in public research outputs. Though more technically demanding, it offers a powerful way to future-proof scientific work.

| Tool                  | Readings                   |
|:----------------------|:---------------------------|
| Code-based Workflows  | @wickham2023, @turrell2025 |
| Literate Programming  | 2                          |
| Version Control       | 3                          |
| Dependency Management | 4                          |
| Containerization      | @boettiger2017             |

: Recommended Readings {#tbl-readings data-quarto-disable-processing="true"}

## Tools as Values in Action

Importantly, these tools do more than facilitate technical accuracy; they teach a way of thinking. Code-based workflows, literate programming, and version/environment management foster habits of precision, documentation, foresight, and openness. They signal that science is not merely about reaching results but about making one's process intelligible and reviewable by others. These are not just technical skills; they reflect core commitments to rigor and accountability in research.

By teaching these tools explicitly, and modeling their use, graduate programs can help students build workflows that are not only reproducible but also resilient, transferable, and collaborative. In a world where scientific claims are increasingly scrutinized, these capacities are not optional. They are part of what it means to do science well.

# Barriers to Implementation

While open science tools have become more powerful and accessible, their integration into graduate training still faces real obstacles. If the benefits are so widely recognized, why haven’t practices like preregistration, reproducible code, and registered reports become standard in clinical psychology programs? The issue is not a lack of awareness or opposition to transparency itself, but rather the practical constraints and competing demands that shape the training environment.

## The Overload of Early Graduate Training

First-year clinical psychology students are often asked to simultaneously learn statistical theory, coding practices, research design, clinical assessment, ethical frameworks, and therapeutic approaches, while also adjusting to the culture and pace of graduate life. Adding open science to this already full plate can feel overwhelming, particularly if students are still developing basic competencies in programming or data analysis. Even motivated students may struggle to incorporate reproducible workflows when they are still grappling with the syntax of R or the nuances of applied linear modeling.

This challenge is not a reason to abandon open science education. But it is a reason to approach it with pedagogical sensitivity. Training must be scaffolded, not just in content but in cognitive demand. Students benefit most when tools are introduced incrementally, in the context of meaningful research questions, and with clear rationale. A template-based preregistration assignment in a methods course is very different from asking a second-year student to submit a registered report for their thesis without prior modeling or feedback.

## Faculty Constraints and Curricular Inertia

The other half of the equation is the faculty. Many instructors are sympathetic to open science but lack the time, training, or institutional support to revamp existing syllabi. It takes effort to redesign assignments to incorporate code-based reproducibility, especially when one is already stretched thin by teaching, mentoring, clinical supervision, and grant writing. Some faculty may feel underqualified to teach newer tools like R or Quarto, particularly if they themselves were trained in a pre-reproducibility era. Others may support transparency in principle but worry that emphasizing rigid practices will stifle student creativity or delay progress on publishable work.

Addressing these constraints will require departmental leadership and collective commitment. Open science cannot rest solely on individual faculty initiative. Programs should offer faculty development resources, encourage team-teaching or co-mentoring across skill sets, and provide ready-made curricular materials that lower the barrier to entry. Faculty need permission and support to evolve alongside the science.

## The Messiness of Real Research

Even when training and support are available, real research often refuses to fit neatly into the boxes provided by preregistration templates [@nosek2019]. This is especially true for students who are still learning statistics and may not yet have the knowledge to specify appropriate analytic methods in advance. Those working with secondary data, developing new measures, or conducting qualitative research may also find it difficult to anticipate all decisions ahead of time. Many studies evolve in response to unexpected data patterns, reviewer feedback, or shifts in scope. Rigid preregistration requirements can backfire if they encourage box-checking over thoughtful planning, discourage exploration, or promote the false impression that research is a linear process rather than an iterative one.

The solution is not to abandon preregistration, but to teach it as a flexible tool rather than a rigid requirement. Not every project is a good fit for full preregistration. Replication studies or work that builds on well-established methods and literatures are often well suited to prespecifying hypotheses and analyses. In contrast, a student's first exploration of a new topic, dataset, or method may be more appropriately framed as exploratory from the outset. Students should understand that deviations from a preregistration are not failures, but opportunities to document learning and refine their reasoning. They should be taught to distinguish clearly between confirmatory and exploratory analyses, to explain changes transparently, and to see preregistration as a tool for clarifying thought, not constraining it. Similarly, registered reports, while powerful, are not appropriate for every project. Their value lies not in universal applicability, but in setting a gold standard for transparency and rigor in studies where hypotheses and analyses can be clearly prespecified.

## The Perfectionism Trap

Finally, one subtle but significant barrier is the culture of perfectionism that can pervade graduate training. Open science, when framed in overly idealistic terms, can unintentionally reinforce the belief that a "good" scientist is one who always writes clean code, anticipates every contingency, and adheres flawlessly to every preregistered plan. For students already struggling with imposter syndrome, this can breed anxiety and paralysis rather than empowerment. Open science is not about perfection, it is about clarity, humility, and continuous improvement. Faculty must actively model this mindset: by sharing imperfect code, troubleshooting errors in front of students, admitting past mistakes, and making their reasoning visible [@strand2025]. Students benefit more from seeing a messy, well-documented workflow than from receiving a polished but opaque one. A culture of openness begins not with mastery, but with modeling vulnerability and growth.

# Conclusion

Van Til and colleagues have provided a valuable service by empirically documenting what many open science advocates in clinical psychology have long suspected: while interest in transparency and rigor is growing, graduate training has yet to catch up. Their registered report offers both a clear-eyed assessment of the current state and a hopeful reminder that students are eager to engage with these topics when given the opportunity.

But awareness is only the first step. If we want open science to take root in clinical psychology, we must invest in the infrastructure, pedagogy, and culture that make transparency not just possible, but normative. That means treating reproducibility as a core competency, not a niche skill. It means integrating preregistration, code-based analysis, and material sharing into the curriculum early and often. It means supporting faculty with training, templates, and time. And it means creating room for nuance, acknowledging that transparency can look different across projects and that imperfection is not a flaw but a feature of honest science.

Open science is a practice we choose, repeatedly, in the face of complexity, uncertainty, and time pressure. It is a commitment to doing science in a way that is intelligible to others and accountable to the communities we serve. For clinical psychological science, that commitment is essential, and so is the work of teaching it. Open science education is how that commitment becomes sustainable. The path forward is not about mandating any single tool or template. It is about cultivating a scientific environment in which students and faculty alike feel empowered to clearly communicate what they did, why they did it, and how others can evaluate or build upon it. That kind of culture begins in the classroom, is reinforced through mentorship, and is sustained by institutions that value good science by rewarding transparency, rigor, and process.

# Acknowledgments

This manuscript was written with the assistance of a large language model (ChatGPT 4o). Specifically, the model was used to refine an initial outline created by the author and to assist with iterative revisions aimed at improving clarity and conciseness. The author has independently verified the accuracy, validity, and appropriateness of all substantive content. As with any tool, LLMs may introduce errors or bias, and the author bears sole responsibility for the final text.

# Author Contributions

Conceptualization: J. Girard; Writing -- Original Draft Preparation: J. Girard.

# Conflicts of Interest

The author declares that there were no conflicts of interest with respect to the authorship or the publication of this article.

# References

::: {#refs}
:::
